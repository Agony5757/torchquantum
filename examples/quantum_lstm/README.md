
# Quantum LSTM


Before the rise of the transformer, recurrent neural networks, especially Long Short-Term Memory (LSTM), were the most successful techniques for generating and analyzing sequential data. LSTM uses a combination of “memory” and “statefulness” tricks to understand which parts of the input are relevant to compute the output.


![LSTM architecture](https://www.researchgate.net/profile/Savvas-Varsamopoulos/publication/329362532/figure/fig5/AS:699592479870977@1543807253596/Structure-of-the-LSTM-cell-and-equations-that-describe-the-gates-of-an-LSTM-cell.jpg)

To convert classical LSTM to quantum-enhanced LSTM (QLSTM) each of the classical linear operations Wf, Wi, WC, and Wo is replaced by a hybrid quantum-classical component that consists of a Variational Quantum Circuit sandwiched between classical layers.

## Authors

- [@MohammadrezaTavasoli](https://github.com/MohammadrezaTavasoli)

